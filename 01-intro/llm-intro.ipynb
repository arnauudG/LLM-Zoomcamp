{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d6212262-0a73-4752-b557-a2806e53c0ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import json\n",
    "from datetime import date\n",
    "from openai import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "51e8c5de-7289-4e95-887a-c4d3ea633b32",
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2df09b42-a6c3-4659-b1cc-ef5512d368e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = OpenAI(api_key = os.environ['OPENAI_API_KEY'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "347682ef-dc64-4ad7-9803-c2568cdc01d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "q = 'How do I run Kafka?'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "63cc6c25-6262-42fb-a92f-53e972d59e17",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_response(client, model, system_context, assistant_context, user_context):\n",
    "    \n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"system\", \"content\": system_context,\n",
    "                \"role\": \"assistant\", \"content\": assistant_context,\n",
    "                \"role\": \"user\", \"content\": user_context\n",
    "            }\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    output = response.choices[0].message.content\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "45e63fab-2234-4fe7-837c-f55866a78390",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To run Kafka, you need to follow these steps:\n",
      "\n",
      "1. Install Kafka on your system by downloading the Kafka binaries from the Apache Kafka website.\n",
      "2. Unzip the downloaded file to a directory on your system.\n",
      "3. Start Zookeeper, which is required for Kafka to run. You can start Zookeeper by running the following command in the Kafka directory:\n",
      "\n",
      "``` \n",
      "bin/zookeeper-server-start.sh config/zookeeper.properties \n",
      "```\n",
      "\n",
      "4. Start the Kafka server by running the following command in the Kafka directory:\n",
      "\n",
      "```\n",
      "bin/kafka-server-start.sh config/server.properties\n",
      "```\n",
      "\n",
      "5. Create a topic in Kafka by running the following command:\n",
      "\n",
      "```\n",
      "bin/kafka-topics.sh --create --topic myTopic --bootstrap-server localhost:9092 --partitions 1 --replication-factor 1\n",
      "```\n",
      "\n",
      "6. Produce messages to the Kafka topic by running the following command:\n",
      "\n",
      "```\n",
      "bin/kafka-console-producer.sh --topic myTopic --bootstrap-server localhost:9092\n",
      "```\n",
      "\n",
      "7. Consume messages from the Kafka topic by running the following command:\n",
      "\n",
      "```\n",
      "bin/kafka-console-consumer.sh --topic myTopic --bootstrap-server localhost:9092 --from-beginning\n",
      "```\n",
      "\n",
      "These steps will help you set up and run Kafka on your system.\n"
     ]
    }
   ],
   "source": [
    "print(get_response(client,\n",
    "             model=\"gpt-3.5-turbo\",\n",
    "             system_context = \"\",\n",
    "             assistant_context = \"\",\n",
    "             user_context = q))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1300505d-bb5c-4062-a71d-af3850f051ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import minsearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2bb5ac30-344d-4594-b39f-2151828644d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing \n",
    "with open('../documents.json', 'rt') as f_in:\n",
    "    raw_documents = json.load(f_in)\n",
    "    docs = []\n",
    "    for course_dict in raw_documents:\n",
    "        for doc in course_dict['documents']:\n",
    "            doc[\"course\"] = course_dict['course']\n",
    "            docs.append(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "45fcf7a5-f624-4362-b8e6-37ee87ca4136",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<minsearch.Index at 0x70637344e8f0>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index = minsearch.Index(\n",
    "    text_fields = [\"text\", \"section\", \"question\"],\n",
    "    keyword_fields = [\"course\"]\n",
    ")\n",
    "index.fit(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0bbe5ebc-ac49-446b-ad88-df17434e1015",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search(q, index):\n",
    "\n",
    "    boost = {'question': 3.0, 'text': 1.0}\n",
    "    \n",
    "    docs_retrieved = index.search(\n",
    "        query = q,\n",
    "        filter_dict={'course': 'data-engineering-zoomcamp'},\n",
    "        boost_dict = boost,\n",
    "        num_results = 30\n",
    "    )\n",
    "\n",
    "    return docs_retrieved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e83f0cd4-1174-4938-bab9-ff3f6a2fcbdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'text': 'In the project directory, run:\\njava -cp build/libs/<jar_name>-1.0-SNAPSHOT.jar:out src/main/java/org/example/JsonProducer.java', 'section': 'Module 6: streaming with kafka', 'question': 'Java Kafka: How to run producer/consumer/kstreams/etc in terminal', 'course': 'data-engineering-zoomcamp'}, {'text': \"Solution from Alexey: create a virtual environment and run requirements.txt and the python files in that environment.\\nTo create a virtual env and install packages (run only once)\\npython -m venv env\\nsource env/bin/activate\\npip install -r ../requirements.txt\\nTo activate it (you'll need to run it every time you need the virtual env):\\nsource env/bin/activate\\nTo deactivate it:\\ndeactivate\\nThis works on MacOS, Linux and Windows - but for Windows the path is slightly different (it's env/Scripts/activate)\\nAlso the virtual environment should be created only to run the python file. Docker images should first all be up and running.\", 'section': 'Module 6: streaming with kafka', 'question': 'Module “kafka” not found when trying to run producer.py', 'course': 'data-engineering-zoomcamp'}, {'text': 'Run this command in terminal in the same directory (/docker/spark):\\nchmod +x build.sh', 'section': 'Module 6: streaming with kafka', 'question': 'Python Kafka: ./build.sh: Permission denied Error', 'course': 'data-engineering-zoomcamp'}, {'text': 'If you have this error, it most likely that your kafka broker docker container is not working.\\nUse docker ps to confirm\\nThen in the docker compose yaml file folder, run docker compose up -d to start all the instances.', 'section': 'Module 6: streaming with kafka', 'question': 'kafka.errors.NoBrokersAvailable: NoBrokersAvailable', 'course': 'data-engineering-zoomcamp'}, {'text': 'Start a new terminal\\nRun: docker ps\\nCopy the CONTAINER ID of the spark-master container\\nRun: docker exec -it <spark_master_container_id> bash\\nRun: cat logs/spark-master.out\\nCheck for the log when the error happened\\nGoogle the error message from there', 'section': 'Module 6: streaming with kafka', 'question': 'Python Kafka: ./spark-submit.sh streaming.py - How to check why Spark master connection fails', 'course': 'data-engineering-zoomcamp'}, {'text': 'You can check the version of your local spark using spark-submit --version. In the build.sh file of the Python folder, make sure that SPARK_VERSION matches your local version. Similarly, make sure the pyspark you pip installed also matches this version.', 'section': 'Module 6: streaming with kafka', 'question': 'How do I check compatibility of local and container Spark versions?', 'course': 'data-engineering-zoomcamp'}, {'text': 'confluent-kafka: `pip install confluent-kafka` or `conda install conda-forge::python-confluent-kafka`\\nfastavro: pip install fastavro\\nAbhirup Ghosh\\nCan install Faust Library for Module 6 Python Version due to dependency conflicts?\\nThe Faust repository and library is no longer maintained - https://github.com/robinhood/faust\\nIf you do not know Java, you now have the option to follow the Python Videos 6.13 & 6.14 here https://www.youtube.com/watch?v=BgAlVknDFlQ&list=PL3MmuxUbc_hJed7dXYoJw8DoCuVHhGEQb&index=80  and follow the RedPanda Python version here https://github.com/DataTalksClub/data-engineering-zoomcamp/tree/main/06-streaming/python/redpanda_example - NOTE: I highly recommend watching the Java videos to understand the concept of streaming but you can skip the coding parts - all will become clear when you get to the Python videos and RedPanda files.', 'section': 'Module 6: streaming with kafka', 'question': 'Python Kafka: Installing dependencies for python3 06-streaming/python/avro_example/producer.py', 'course': 'data-engineering-zoomcamp'}, {'text': \"Answer: To run the provided code, ensure that the 'dlt[duckdb]' package is installed. You can do this by executing the provided installation command: !pip install dlt[duckdb]. If you’re doing it locally, be sure to also have duckdb pip installed (even before the duckdb package is loaded).\", 'section': 'Workshop 1 - dlthub', 'question': 'How do I install the necessary dependencies to run the code?', 'course': 'data-engineering-zoomcamp'}, {'text': 'Situation: in VS Code, usually there will be a triangle icon next to each test. I couldn’t see it at first and had to do some fixes.\\nSolution:\\n(Source)\\nVS Code\\n→ Explorer (first icon on the left navigation bar)\\n→ JAVA PROJECTS (bottom collapsable)\\n→  icon next in the rightmost position to JAVA PROJECTS\\n→  clean Workspace\\n→ Confirm by clicking Reload and Delete\\nNow you will be able to see the triangle icon next to each test like what you normally see in python tests.\\nE.g.:\\nYou can also add classes and packages in this window instead of creating files in the project directory', 'section': 'Module 6: streaming with kafka', 'question': 'Java Kafka: Tests are not picked up in VSCode', 'course': 'data-engineering-zoomcamp'}, {'text': 'In my set up, all of the dependencies listed in gradle.build were not installed in <project_name>-1.0-SNAPSHOT.jar.\\nSolution:\\nIn build.gradle file, I added the following at the end:\\nshadowJar {\\narchiveBaseName = \"java-kafka-rides\"\\narchiveClassifier = \\'\\'\\n}\\nAnd then in the command line ran ‘gradle shadowjar’, and run the script from java-kafka-rides-1.0-SNAPSHOT.jar created by the shadowjar', 'section': 'Module 6: streaming with kafka', 'question': 'Java Kafka: <project_name>-1.0-SNAPSHOT.jar errors: package xxx does not exist even after gradle build', 'course': 'data-engineering-zoomcamp'}, {'text': 'tip:As the videos have low audio so I downloaded them and used VLC media player with putting the audio to the max 200% of original audio and the audio became quite good or try to use auto caption generated on Youtube directly.\\nKafka Python Videos - Rides.csv\\nThere is no clear explanation of the rides.csv data that the producer.py python programs use. You can find that here https://raw.githubusercontent.com/DataTalksClub/data-engineering-zoomcamp/2bd33e89906181e424f7b12a299b70b19b7cfcd5/week_6_stream_processing/python/resources/rides.csv.', 'section': 'Module 6: streaming with kafka', 'question': 'Kafka- python videos have low audio and hard to follow up', 'course': 'data-engineering-zoomcamp'}, {'text': 'In Confluent Cloud:\\nEnvironment → default (or whatever you named your environment as) → The right navigation bar →  “Stream Governance API” →  The URL under “Endpoint”\\nAnd create credentials from Credentials section below it', 'section': 'Module 6: streaming with kafka', 'question': 'Confluent Kafka: Where can I find schema registry URL?', 'course': 'data-engineering-zoomcamp'}, {'text': 'For example, when running JsonConsumer.java, got:\\nConsuming form kafka started\\nRESULTS:::0\\nRESULTS:::0\\nRESULTS:::0\\nOr when running JsonProducer.java, got:\\nException in thread \"main\" java.util.concurrent.ExecutionException: org.apache.kafka.common.errors.SaslAuthenticationException: Authentication failed\\nSolution:\\nMake sure in the scripts in src/main/java/org/example/ that you are running (e.g. JsonConsumer.java, JsonProducer.java), the StreamsConfig.BOOTSTRAP_SERVERS_CONFIG is the correct server url (e.g. europe-west3 from example vs europe-west2)\\nMake sure cluster key and secrets are updated in src/main/java/org/example/Secrets.java (KAFKA_CLUSTER_KEY and KAFKA_CLUSTER_SECRET)', 'section': 'Module 6: streaming with kafka', 'question': 'Java Kafka: When running the producer/consumer/etc java scripts, no results retrieved or no message sent', 'course': 'data-engineering-zoomcamp'}, {'text': 'According to https://github.com/dpkp/kafka-python/\\n“DUE TO ISSUES WITH RELEASES, IT IS SUGGESTED TO USE https://github.com/wbarnha/kafka-python-ng FOR THE TIME BEING”\\nUse pip install kafka-python-ng instead', 'section': 'Project', 'question': 'How to fix the error \"ModuleNotFoundError: No module named \\'kafka.vendor.six.moves\\'\"?', 'course': 'data-engineering-zoomcamp'}, {'text': 'If you get an error while running the command python3 stream.py worker\\nRun pip uninstall kafka-python\\nThen run pip install kafka-python==1.4.6\\nWhat is the use of  Redpanda ?\\nRedpanda: Redpanda is built on top of the Raft consensus algorithm and is designed as a high-performance, low-latency alternative to Kafka. It uses a log-centric architecture similar to Kafka but with different underlying principles.\\nRedpanda is a powerful, yet simple, and cost-efficient streaming data platform that is compatible with Kafka® APIs while eliminating Kafka complexity.', 'section': 'Module 6: streaming with kafka', 'question': 'Error while running python3 stream.py worker', 'course': 'data-engineering-zoomcamp'}, {'text': 'Ankush said we can focus on horizontal scaling option.\\n“think of scaling in terms of scaling from consumer end. Or consuming message via horizontal scaling”', 'section': 'Module 6: streaming with kafka', 'question': 'Kafka homwork Q3, there are options that support scaling concept more than the others:', 'course': 'data-engineering-zoomcamp'}, {'text': 'Restarting all services worked for me:\\ndocker-compose down\\ndocker-compose up', 'section': 'Module 6: streaming with kafka', 'question': 'Python Kafka: ‘KafkaTimeoutError: Failed to update metadata after 60.0 secs.’ when running stream-example/producer.py', 'course': 'data-engineering-zoomcamp'}, {'text': \"✅SOLUTION: pip install confluent-kafka[avro].\\nFor some reason, Conda also doesn't include this when installing confluent-kafka via pip.\\nMore sources on Anaconda and confluent-kafka issues:\\nhttps://github.com/confluentinc/confluent-kafka-python/issues/590\\nhttps://github.com/confluentinc/confluent-kafka-python/issues/1221\\nhttps://stackoverflow.com/questions/69085157/cannot-import-producer-from-confluent-kafka\", 'section': 'Module 6: streaming with kafka', 'question': \"ModuleNotFoundError: No module named 'avro'\", 'course': 'data-engineering-zoomcamp'}, {'text': 'While following tutorial 13.2 , when running ./spark-submit.sh streaming.py, encountered the following error:\\n…\\n24/03/11 09:48:36 INFO StandaloneAppClient$ClientEndpoint: Connecting to master spark://localhost:7077...\\n24/03/11 09:48:36 INFO TransportClientFactory: Successfully created connection to localhost/127.0.0.1:7077 after 10 ms (0 ms spent in bootstraps)\\n24/03/11 09:48:54 WARN GarbageCollectionMetrics: To enable non-built-in garbage collector(s) List(G1 Concurrent GC), users should configure it(them) to spark.eventLog.gcMetrics.youngGenerationGarbageCollectors or spark.eventLog.gcMetrics.oldGenerationGarbageCollectors\\n24/03/11 09:48:56 INFO StandaloneAppClient$ClientEndpoint: Connecting to master spark://localhost:7077…\\n24/03/11 09:49:16 INFO StandaloneAppClient$ClientEndpoint: Connecting to master spark://localhost:7077...\\n24/03/11 09:49:36 WARN StandaloneSchedulerBackend: Application ID is not initialized yet.\\n24/03/11 09:49:36 ERROR StandaloneSchedulerBackend: Application has been killed. Reason: All masters are unresponsive! Giving up.\\n…\\npy4j.protocol.Py4JJavaError: An error occurred while calling None.org.apache.spark.sql.SparkSession.\\n: java.lang.IllegalStateException: Cannot call methods on a stopped SparkContext.\\n…\\nSolution:\\nDowngrade your local PySpark to 3.3.1 (same as Dockerfile)\\nThe reason for the failed connection in my case was the mismatch of PySpark versions. You can see that from the logs of spark-master in the docker container.\\nSolution 2:\\nCheck what Spark version your local machine has\\npyspark –version\\nspark-submit –version\\nAdd your version to SPARK_VERSION in build.sh', 'section': 'Module 6: streaming with kafka', 'question': 'Python Kafka: ./spark-submit.sh streaming.py - ERROR StandaloneSchedulerBackend: Application has been killed. Reason: All masters are unresponsive! Giving up.', 'course': 'data-engineering-zoomcamp'}, {'text': 'Make sure your java version is 11 or 8.\\nCheck your version by:\\njava --version\\nCheck all your versions by:\\n/usr/libexec/java_home -V\\nIf you already have got java 11 but just not selected as default, select the specific version by:\\nexport JAVA_HOME=$(/usr/libexec/java_home -v 11.0.22)\\n(or other version of 11)', 'section': 'Module 6: streaming with kafka', 'question': 'Python Kafka: ./spark-submit.sh streaming.py Error: py4j.protocol.Py4JJavaError: An error occurred while calling None.org.apache.spark.api.java.JavaSparkContext.', 'course': 'data-engineering-zoomcamp'}, {'text': 'After you create a GitHub account, you should clone the course repo to your local machine using the process outlined in this video: Git for Everybody: How to Clone a Repository from GitHub\\nHaving this local repository on your computer will make it easy for you to access the instructors’ code and make pull requests (if you want to add your own notes or make changes to the course content).\\nYou will probably also create your own repositories that host your notes, versions of your file, to do this. Here is a great tutorial that shows you how to do this: https://www.atlassian.com/git/tutorials/setting-up-a-repository\\nRemember to ignore large database, .csv, and .gz files, and other files that should not be saved to a repository. Use .gitignore for this: https://www.atlassian.com/git/tutorials/saving-changes/gitignore NEVER store passwords or keys in a git repo (even if that repo is set to private).\\nThis is also a great resource: https://dangitgit.com/', 'section': 'General course-related questions', 'question': 'How do I use Git / GitHub for this course?', 'course': 'data-engineering-zoomcamp'}, {'text': \"My taxi data was loaded into gcs with etl_web_to_gcs.py script that converts csv data into parquet. Then I placed raw data trips into external tables and when I executed dbt run I got an error message: Parquet column 'passenger_count' has type INT64 which does not match the target cpp_type DOUBLE. It is because several columns in files have different formats of data.\\nWhen I added df[col] = df[col].astype('Int64') transformation to the columns: passenger_count, payment_type, RatecodeID, VendorID, trip_type it went ok. Several people also faced this error and more about it you can read on the slack channel.\", 'section': 'Module 4: analytics engineering with dbt', 'question': 'BigQuery returns an error when i try to run ‘dbt run’:', 'course': 'data-engineering-zoomcamp'}, {'text': 'Python 3.12.1, is not compatible with kafka-python-2.0.2. Therefore, instead of running \"pip install kafka-python\", you can resolve the issue by using \"pip install git+https://github.com/dpkp/kafka-python.git\". If you have already installed kafka-python, you need to run \"pip uninstall kafka-python\" before executing \"pip install git+https://github.com/dpkp/kafka-python.git\" to resolve the compatibility issue.\\nQ:In the Mage pipeline, individual blocks run successfully. However, when executing the pipeline as a whole, some blocks fail.\\nA: I have the following key-value pair in io_config.yaml file configured but still Mage blocks failed to generate OAuth and authenticate with GCP: GOOGLE_SERVICE_ACC_KEY_FILEPATH: \"{{ env_var(\\'GCP_CREDENTIALS\\') }}\". The GCP_CREDENTIALS variable holds the full path to the service account key\\'s JSON file. Adding the following line within the failed code block resolved the issue: os.environ[\\'GOOGLE_APPLICATION_CREDENTIALS\\'] = os.environ.get(\\'GCP_CREDENTIALS\\').\\nThis occurs because the path to profiles.yml is not correctly specified. You can rectify this by:\\n“export DBT_PROFILES_DBT=path/to/profiles.yml”\\nEg., /home/src/magic-zoomcamp/dbt/project_name/\\nDo the similar for DBT_PROJECT_DIR if getting similar issue with dbt_project.yml.\\nOnce DIRs are set,:\\n“dbt debug –config-dir”\\nThis would update your paths. To maintain same path across sessions, use the path variables in your .env file.\\nTo add triggers in mage pipelines via CLI, you can create a trigger of type API, and copy the API links.\\nEg. link: http://localhost:6789/api/pipeline_schedules/10/pipeline_runs/f3a1a4228fc64cfd85295b668c93f3b2\\nThen create a trigger.py as such:\\nimport os\\nimport requests\\nclass MageTrigger:\\nOPTIONS = {\\n\"<pipeline_name>\": {\\n\"trigger_id\": 10,\\n\"key\": \"f3a1a4228fc64cfd85295b668c93f3b2\"\\n}\\n}\\n@staticmethod\\ndef trigger_pipeline(pipeline_name, variables=None):\\ntrigger_id = MageTrigger.OPTIONS[pipeline_name][\"trigger_id\"]\\nkey = MageTrigger.OPTIONS[pipeline_name][\"key\"]\\nendpoint = f\"http://localhost:6789/api/pipeline_schedules/{trigger_id}/pipeline_runs/{key}\"\\nheaders = {\\'Content-Type\\': \\'application/json\\'}\\npayload = {}\\nif variables is not None:\\npayload[\\'pipeline_run\\'] = {\\'variables\\': variables}\\nresponse = requests.post(endpoint, headers=headers, json=payload)\\nreturn response\\nMageTrigger.trigger_pipeline(\"<pipeline_name>\")\\nFinally, after the mage server is up an running, simply this command:\\npython trigger.py from mage directory in terminal.\\nCan I do data partitioning & clustering run by dbt pipeline, or I would need to do this manually in BigQuery afterwards?\\nYou can use this configuration in your DBT model:\\n{\\n\"field\": \"<field name>\",\\n\"data_type\": \"<timestamp | date | datetime | int64>\",\\n\"granularity\": \"<hour | day | month | year>\"\\n# Only required if data_type is \"int64\"\\n\"range\": {\\n\"start\": <int>,\\n\"end\": <int>,\\n\"interval\": <int>\\n}\\n}\\nand for clustering\\n{{\\nconfig(\\nmaterialized = \"table\",\\ncluster_by = \"order_id\",\\n)\\n}}\\nmore details in: https://docs.getdbt.com/reference/resource-configs/bigquery-configs', 'section': 'Triggers in Mage via CLI', 'question': 'Encountering the error \"ModuleNotFoundError: No module named \\'kafka.vendor.six.moves\\'\" when running \"from kafka import KafkaProducer\" in Jupyter Notebook for Module 6 Homework?', 'course': 'data-engineering-zoomcamp'}, {'text': \"Below I have listed some steps I took to rectify this and potentially other minor errors, in Windows:\\nUse the git bash terminal in windows.\\nActivate python venv from git bash: source .venv/Scripts/activate\\nModify the seed_kafka.py file: in the first line, replace python3 with python.\\nNow from git bash, run the seed-kafka cmd. It should work now.\\nAdditional Notes:\\nYou can connect to the RisingWave cluster from Powershell with the command psql -h localhost -p 4566 -d dev -U root , otherwise it asks for a password.\\nThe equivalent of source commands.sh  in Powershell is . .\\\\commands.sh from the workshop directory.\\nHope this can save you from some trouble in case you're doing this workshop on Windows like I am.\\n—--------------------------------------------------------------------------------------\", 'section': 'Workshop 2 - RisingWave', 'question': 'Psycopg2 InternalError: Failed to run the query - when running the seed-kafka command after initial setup.', 'course': 'data-engineering-zoomcamp'}, {'text': 'You need to redefine the python environment variable to that of your user account', 'section': 'Project', 'question': 'How to run python as start up script?', 'course': 'data-engineering-zoomcamp'}, {'text': 'If you get this error, know that you have not built your sparks and juypter images. This images aren’t readily available on dockerHub.\\nIn the spark folder, run ./build.sh from a bash cli to to build all images before running docker compose', 'section': 'Module 6: streaming with kafka', 'question': \"How to fix docker compose error: Error response from daemon: pull access denied for spark-3.3.1, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\", 'course': 'data-engineering-zoomcamp'}, {'text': 'Check Docker Compose File:\\nEnsure that your docker-compose.yaml file is correctly configured with the necessary details for the \"control-center\" service. Check the service name, image name, ports, volumes, environment variables, and any other configurations required for the container to start.\\nOn Mac OSX 12.2.1 (Monterey) I could not start the kafka control center. I opened Docker Desktop and saw docker images still running from week 4, which I did not see when I typed “docker ps.” I deleted them in docker desktop and then had no problem starting up the kafka environment.', 'section': 'Module 6: streaming with kafka', 'question': 'Could not start docker image “control-center” from the docker-compose.yaml file.', 'course': 'data-engineering-zoomcamp'}, {'text': 'Replace psycopg2==2.9.9 with psycopg2-binary in the requirements.txt file [source] [another]\\nWhen you open another terminal to run the psql, remember to do the source command.sh step for each terminal session\\n---------------------------------------------------------------------------------------------', 'section': 'Workshop 2 - RisingWave', 'question': 'Psycopg2 - issues when running stream-kafka script', 'course': 'data-engineering-zoomcamp'}, {'text': 'Change the working directory to the spark directory:\\nif you have setup up your SPARK_HOME variable, use the following;\\ncd %SPARK_HOME%\\nif not, use the following;\\ncd <path to spark installation>\\nCreating a Local Spark Cluster\\nTo start Spark Master:\\nbin\\\\spark-class org.apache.spark.deploy.master.Master --host localhost\\nStarting up a cluster:\\nbin\\\\spark-class org.apache.spark.deploy.worker.Worker spark://localhost:7077 --host localhost', 'section': 'Module 5: pyspark', 'question': 'How to spark standalone cluster is run on windows OS', 'course': 'data-engineering-zoomcamp'}, {'text': 'ImportError: DLL load failed while importing cimpl: The specified module could not be found\\nVerify Python Version:\\nMake sure you are using a compatible version of Python with the Avro library. Check the Python version and compatibility requirements specified by the Avro library documentation.\\n... you may have to load librdkafka-5d2e2910.dll in the code. Add this before importing avro:\\nfrom ctypes import CDLL\\nCDLL(\"C:\\\\\\\\Users\\\\\\\\YOUR_USER_NAME\\\\\\\\anaconda3\\\\\\\\envs\\\\\\\\dtcde\\\\\\\\Lib\\\\\\\\site-packages\\\\\\\\confluent_kafka.libs\\\\librdkafka-5d2e2910.dll\")\\nIt seems that the error may occur depending on the OS and python version installed.\\nALTERNATIVE:\\nImportError: DLL load failed while importing cimpl\\n✅SOLUTION: $env:CONDA_DLL_SEARCH_MODIFICATION_ENABLE=1 in Powershell.\\nYou need to set this DLL manually in Conda Env.\\nSource: https://githubhot.com/repo/confluentinc/confluent-kafka-python/issues/1186?page=2', 'section': 'Module 6: streaming with kafka', 'question': 'Error importing cimpl dll when running avro examples', 'course': 'data-engineering-zoomcamp'}]\n"
     ]
    }
   ],
   "source": [
    "print(search(q, index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fcd8ca8d-f470-42c3-a9c3-7a261de3b9eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_context(docs_retrieved):\n",
    "\n",
    "    context = \"\"\n",
    "    for doc_retrieved in docs_retrieved:\n",
    "        context = context + f\"Section: {doc_retrieved['section']}\\nQuestion: {doc_retrieved['question']}\\nAnswer: {doc_retrieved['text']}\\n\\n\"\n",
    "\n",
    "    return context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "16233eff-07f4-43ce-b498-da8d8ee20d17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Section: Module 6: streaming with kafka\n",
      "Question: Java Kafka: How to run producer/consumer/kstreams/etc in terminal\n",
      "Answer: In the project directory, run:\n",
      "java -cp build/libs/<jar_name>-1.0-SNAPSHOT.jar:out src/main/java/org/example/JsonProducer.java\n",
      "\n",
      "Section: Module 6: streaming with kafka\n",
      "Question: Module “kafka” not found when trying to run producer.py\n",
      "Answer: Solution from Alexey: create a virtual environment and run requirements.txt and the python files in that environment.\n",
      "To create a virtual env and install packages (run only once)\n",
      "python -m venv env\n",
      "source env/bin/activate\n",
      "pip install -r ../requirements.txt\n",
      "To activate it (you'll need to run it every time you need the virtual env):\n",
      "source env/bin/activate\n",
      "To deactivate it:\n",
      "deactivate\n",
      "This works on MacOS, Linux and Windows - but for Windows the path is slightly different (it's env/Scripts/activate)\n",
      "Also the virtual environment should be created only to run the python file. Docker images should first all be up and running.\n",
      "\n",
      "Section: Module 6: streaming with kafka\n",
      "Question: Python Kafka: ./build.sh: Permission denied Error\n",
      "Answer: Run this command in terminal in the same directory (/docker/spark):\n",
      "chmod +x build.sh\n",
      "\n",
      "Section: Module 6: streaming with kafka\n",
      "Question: kafka.errors.NoBrokersAvailable: NoBrokersAvailable\n",
      "Answer: If you have this error, it most likely that your kafka broker docker container is not working.\n",
      "Use docker ps to confirm\n",
      "Then in the docker compose yaml file folder, run docker compose up -d to start all the instances.\n",
      "\n",
      "Section: Module 6: streaming with kafka\n",
      "Question: Python Kafka: ./spark-submit.sh streaming.py - How to check why Spark master connection fails\n",
      "Answer: Start a new terminal\n",
      "Run: docker ps\n",
      "Copy the CONTAINER ID of the spark-master container\n",
      "Run: docker exec -it <spark_master_container_id> bash\n",
      "Run: cat logs/spark-master.out\n",
      "Check for the log when the error happened\n",
      "Google the error message from there\n",
      "\n",
      "Section: Module 6: streaming with kafka\n",
      "Question: How do I check compatibility of local and container Spark versions?\n",
      "Answer: You can check the version of your local spark using spark-submit --version. In the build.sh file of the Python folder, make sure that SPARK_VERSION matches your local version. Similarly, make sure the pyspark you pip installed also matches this version.\n",
      "\n",
      "Section: Module 6: streaming with kafka\n",
      "Question: Python Kafka: Installing dependencies for python3 06-streaming/python/avro_example/producer.py\n",
      "Answer: confluent-kafka: `pip install confluent-kafka` or `conda install conda-forge::python-confluent-kafka`\n",
      "fastavro: pip install fastavro\n",
      "Abhirup Ghosh\n",
      "Can install Faust Library for Module 6 Python Version due to dependency conflicts?\n",
      "The Faust repository and library is no longer maintained - https://github.com/robinhood/faust\n",
      "If you do not know Java, you now have the option to follow the Python Videos 6.13 & 6.14 here https://www.youtube.com/watch?v=BgAlVknDFlQ&list=PL3MmuxUbc_hJed7dXYoJw8DoCuVHhGEQb&index=80  and follow the RedPanda Python version here https://github.com/DataTalksClub/data-engineering-zoomcamp/tree/main/06-streaming/python/redpanda_example - NOTE: I highly recommend watching the Java videos to understand the concept of streaming but you can skip the coding parts - all will become clear when you get to the Python videos and RedPanda files.\n",
      "\n",
      "Section: Workshop 1 - dlthub\n",
      "Question: How do I install the necessary dependencies to run the code?\n",
      "Answer: Answer: To run the provided code, ensure that the 'dlt[duckdb]' package is installed. You can do this by executing the provided installation command: !pip install dlt[duckdb]. If you’re doing it locally, be sure to also have duckdb pip installed (even before the duckdb package is loaded).\n",
      "\n",
      "Section: Module 6: streaming with kafka\n",
      "Question: Java Kafka: Tests are not picked up in VSCode\n",
      "Answer: Situation: in VS Code, usually there will be a triangle icon next to each test. I couldn’t see it at first and had to do some fixes.\n",
      "Solution:\n",
      "(Source)\n",
      "VS Code\n",
      "→ Explorer (first icon on the left navigation bar)\n",
      "→ JAVA PROJECTS (bottom collapsable)\n",
      "→  icon next in the rightmost position to JAVA PROJECTS\n",
      "→  clean Workspace\n",
      "→ Confirm by clicking Reload and Delete\n",
      "Now you will be able to see the triangle icon next to each test like what you normally see in python tests.\n",
      "E.g.:\n",
      "You can also add classes and packages in this window instead of creating files in the project directory\n",
      "\n",
      "Section: Module 6: streaming with kafka\n",
      "Question: Java Kafka: <project_name>-1.0-SNAPSHOT.jar errors: package xxx does not exist even after gradle build\n",
      "Answer: In my set up, all of the dependencies listed in gradle.build were not installed in <project_name>-1.0-SNAPSHOT.jar.\n",
      "Solution:\n",
      "In build.gradle file, I added the following at the end:\n",
      "shadowJar {\n",
      "archiveBaseName = \"java-kafka-rides\"\n",
      "archiveClassifier = ''\n",
      "}\n",
      "And then in the command line ran ‘gradle shadowjar’, and run the script from java-kafka-rides-1.0-SNAPSHOT.jar created by the shadowjar\n",
      "\n",
      "Section: Module 6: streaming with kafka\n",
      "Question: Kafka- python videos have low audio and hard to follow up\n",
      "Answer: tip:As the videos have low audio so I downloaded them and used VLC media player with putting the audio to the max 200% of original audio and the audio became quite good or try to use auto caption generated on Youtube directly.\n",
      "Kafka Python Videos - Rides.csv\n",
      "There is no clear explanation of the rides.csv data that the producer.py python programs use. You can find that here https://raw.githubusercontent.com/DataTalksClub/data-engineering-zoomcamp/2bd33e89906181e424f7b12a299b70b19b7cfcd5/week_6_stream_processing/python/resources/rides.csv.\n",
      "\n",
      "Section: Module 6: streaming with kafka\n",
      "Question: Confluent Kafka: Where can I find schema registry URL?\n",
      "Answer: In Confluent Cloud:\n",
      "Environment → default (or whatever you named your environment as) → The right navigation bar →  “Stream Governance API” →  The URL under “Endpoint”\n",
      "And create credentials from Credentials section below it\n",
      "\n",
      "Section: Module 6: streaming with kafka\n",
      "Question: Java Kafka: When running the producer/consumer/etc java scripts, no results retrieved or no message sent\n",
      "Answer: For example, when running JsonConsumer.java, got:\n",
      "Consuming form kafka started\n",
      "RESULTS:::0\n",
      "RESULTS:::0\n",
      "RESULTS:::0\n",
      "Or when running JsonProducer.java, got:\n",
      "Exception in thread \"main\" java.util.concurrent.ExecutionException: org.apache.kafka.common.errors.SaslAuthenticationException: Authentication failed\n",
      "Solution:\n",
      "Make sure in the scripts in src/main/java/org/example/ that you are running (e.g. JsonConsumer.java, JsonProducer.java), the StreamsConfig.BOOTSTRAP_SERVERS_CONFIG is the correct server url (e.g. europe-west3 from example vs europe-west2)\n",
      "Make sure cluster key and secrets are updated in src/main/java/org/example/Secrets.java (KAFKA_CLUSTER_KEY and KAFKA_CLUSTER_SECRET)\n",
      "\n",
      "Section: Project\n",
      "Question: How to fix the error \"ModuleNotFoundError: No module named 'kafka.vendor.six.moves'\"?\n",
      "Answer: According to https://github.com/dpkp/kafka-python/\n",
      "“DUE TO ISSUES WITH RELEASES, IT IS SUGGESTED TO USE https://github.com/wbarnha/kafka-python-ng FOR THE TIME BEING”\n",
      "Use pip install kafka-python-ng instead\n",
      "\n",
      "Section: Module 6: streaming with kafka\n",
      "Question: Error while running python3 stream.py worker\n",
      "Answer: If you get an error while running the command python3 stream.py worker\n",
      "Run pip uninstall kafka-python\n",
      "Then run pip install kafka-python==1.4.6\n",
      "What is the use of  Redpanda ?\n",
      "Redpanda: Redpanda is built on top of the Raft consensus algorithm and is designed as a high-performance, low-latency alternative to Kafka. It uses a log-centric architecture similar to Kafka but with different underlying principles.\n",
      "Redpanda is a powerful, yet simple, and cost-efficient streaming data platform that is compatible with Kafka® APIs while eliminating Kafka complexity.\n",
      "\n",
      "Section: Module 6: streaming with kafka\n",
      "Question: Kafka homwork Q3, there are options that support scaling concept more than the others:\n",
      "Answer: Ankush said we can focus on horizontal scaling option.\n",
      "“think of scaling in terms of scaling from consumer end. Or consuming message via horizontal scaling”\n",
      "\n",
      "Section: Module 6: streaming with kafka\n",
      "Question: Python Kafka: ‘KafkaTimeoutError: Failed to update metadata after 60.0 secs.’ when running stream-example/producer.py\n",
      "Answer: Restarting all services worked for me:\n",
      "docker-compose down\n",
      "docker-compose up\n",
      "\n",
      "Section: Module 6: streaming with kafka\n",
      "Question: ModuleNotFoundError: No module named 'avro'\n",
      "Answer: ✅SOLUTION: pip install confluent-kafka[avro].\n",
      "For some reason, Conda also doesn't include this when installing confluent-kafka via pip.\n",
      "More sources on Anaconda and confluent-kafka issues:\n",
      "https://github.com/confluentinc/confluent-kafka-python/issues/590\n",
      "https://github.com/confluentinc/confluent-kafka-python/issues/1221\n",
      "https://stackoverflow.com/questions/69085157/cannot-import-producer-from-confluent-kafka\n",
      "\n",
      "Section: Module 6: streaming with kafka\n",
      "Question: Python Kafka: ./spark-submit.sh streaming.py - ERROR StandaloneSchedulerBackend: Application has been killed. Reason: All masters are unresponsive! Giving up.\n",
      "Answer: While following tutorial 13.2 , when running ./spark-submit.sh streaming.py, encountered the following error:\n",
      "…\n",
      "24/03/11 09:48:36 INFO StandaloneAppClient$ClientEndpoint: Connecting to master spark://localhost:7077...\n",
      "24/03/11 09:48:36 INFO TransportClientFactory: Successfully created connection to localhost/127.0.0.1:7077 after 10 ms (0 ms spent in bootstraps)\n",
      "24/03/11 09:48:54 WARN GarbageCollectionMetrics: To enable non-built-in garbage collector(s) List(G1 Concurrent GC), users should configure it(them) to spark.eventLog.gcMetrics.youngGenerationGarbageCollectors or spark.eventLog.gcMetrics.oldGenerationGarbageCollectors\n",
      "24/03/11 09:48:56 INFO StandaloneAppClient$ClientEndpoint: Connecting to master spark://localhost:7077…\n",
      "24/03/11 09:49:16 INFO StandaloneAppClient$ClientEndpoint: Connecting to master spark://localhost:7077...\n",
      "24/03/11 09:49:36 WARN StandaloneSchedulerBackend: Application ID is not initialized yet.\n",
      "24/03/11 09:49:36 ERROR StandaloneSchedulerBackend: Application has been killed. Reason: All masters are unresponsive! Giving up.\n",
      "…\n",
      "py4j.protocol.Py4JJavaError: An error occurred while calling None.org.apache.spark.sql.SparkSession.\n",
      ": java.lang.IllegalStateException: Cannot call methods on a stopped SparkContext.\n",
      "…\n",
      "Solution:\n",
      "Downgrade your local PySpark to 3.3.1 (same as Dockerfile)\n",
      "The reason for the failed connection in my case was the mismatch of PySpark versions. You can see that from the logs of spark-master in the docker container.\n",
      "Solution 2:\n",
      "Check what Spark version your local machine has\n",
      "pyspark –version\n",
      "spark-submit –version\n",
      "Add your version to SPARK_VERSION in build.sh\n",
      "\n",
      "Section: Module 6: streaming with kafka\n",
      "Question: Python Kafka: ./spark-submit.sh streaming.py Error: py4j.protocol.Py4JJavaError: An error occurred while calling None.org.apache.spark.api.java.JavaSparkContext.\n",
      "Answer: Make sure your java version is 11 or 8.\n",
      "Check your version by:\n",
      "java --version\n",
      "Check all your versions by:\n",
      "/usr/libexec/java_home -V\n",
      "If you already have got java 11 but just not selected as default, select the specific version by:\n",
      "export JAVA_HOME=$(/usr/libexec/java_home -v 11.0.22)\n",
      "(or other version of 11)\n",
      "\n",
      "Section: General course-related questions\n",
      "Question: How do I use Git / GitHub for this course?\n",
      "Answer: After you create a GitHub account, you should clone the course repo to your local machine using the process outlined in this video: Git for Everybody: How to Clone a Repository from GitHub\n",
      "Having this local repository on your computer will make it easy for you to access the instructors’ code and make pull requests (if you want to add your own notes or make changes to the course content).\n",
      "You will probably also create your own repositories that host your notes, versions of your file, to do this. Here is a great tutorial that shows you how to do this: https://www.atlassian.com/git/tutorials/setting-up-a-repository\n",
      "Remember to ignore large database, .csv, and .gz files, and other files that should not be saved to a repository. Use .gitignore for this: https://www.atlassian.com/git/tutorials/saving-changes/gitignore NEVER store passwords or keys in a git repo (even if that repo is set to private).\n",
      "This is also a great resource: https://dangitgit.com/\n",
      "\n",
      "Section: Module 4: analytics engineering with dbt\n",
      "Question: BigQuery returns an error when i try to run ‘dbt run’:\n",
      "Answer: My taxi data was loaded into gcs with etl_web_to_gcs.py script that converts csv data into parquet. Then I placed raw data trips into external tables and when I executed dbt run I got an error message: Parquet column 'passenger_count' has type INT64 which does not match the target cpp_type DOUBLE. It is because several columns in files have different formats of data.\n",
      "When I added df[col] = df[col].astype('Int64') transformation to the columns: passenger_count, payment_type, RatecodeID, VendorID, trip_type it went ok. Several people also faced this error and more about it you can read on the slack channel.\n",
      "\n",
      "Section: Triggers in Mage via CLI\n",
      "Question: Encountering the error \"ModuleNotFoundError: No module named 'kafka.vendor.six.moves'\" when running \"from kafka import KafkaProducer\" in Jupyter Notebook for Module 6 Homework?\n",
      "Answer: Python 3.12.1, is not compatible with kafka-python-2.0.2. Therefore, instead of running \"pip install kafka-python\", you can resolve the issue by using \"pip install git+https://github.com/dpkp/kafka-python.git\". If you have already installed kafka-python, you need to run \"pip uninstall kafka-python\" before executing \"pip install git+https://github.com/dpkp/kafka-python.git\" to resolve the compatibility issue.\n",
      "Q:In the Mage pipeline, individual blocks run successfully. However, when executing the pipeline as a whole, some blocks fail.\n",
      "A: I have the following key-value pair in io_config.yaml file configured but still Mage blocks failed to generate OAuth and authenticate with GCP: GOOGLE_SERVICE_ACC_KEY_FILEPATH: \"{{ env_var('GCP_CREDENTIALS') }}\". The GCP_CREDENTIALS variable holds the full path to the service account key's JSON file. Adding the following line within the failed code block resolved the issue: os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = os.environ.get('GCP_CREDENTIALS').\n",
      "This occurs because the path to profiles.yml is not correctly specified. You can rectify this by:\n",
      "“export DBT_PROFILES_DBT=path/to/profiles.yml”\n",
      "Eg., /home/src/magic-zoomcamp/dbt/project_name/\n",
      "Do the similar for DBT_PROJECT_DIR if getting similar issue with dbt_project.yml.\n",
      "Once DIRs are set,:\n",
      "“dbt debug –config-dir”\n",
      "This would update your paths. To maintain same path across sessions, use the path variables in your .env file.\n",
      "To add triggers in mage pipelines via CLI, you can create a trigger of type API, and copy the API links.\n",
      "Eg. link: http://localhost:6789/api/pipeline_schedules/10/pipeline_runs/f3a1a4228fc64cfd85295b668c93f3b2\n",
      "Then create a trigger.py as such:\n",
      "import os\n",
      "import requests\n",
      "class MageTrigger:\n",
      "OPTIONS = {\n",
      "\"<pipeline_name>\": {\n",
      "\"trigger_id\": 10,\n",
      "\"key\": \"f3a1a4228fc64cfd85295b668c93f3b2\"\n",
      "}\n",
      "}\n",
      "@staticmethod\n",
      "def trigger_pipeline(pipeline_name, variables=None):\n",
      "trigger_id = MageTrigger.OPTIONS[pipeline_name][\"trigger_id\"]\n",
      "key = MageTrigger.OPTIONS[pipeline_name][\"key\"]\n",
      "endpoint = f\"http://localhost:6789/api/pipeline_schedules/{trigger_id}/pipeline_runs/{key}\"\n",
      "headers = {'Content-Type': 'application/json'}\n",
      "payload = {}\n",
      "if variables is not None:\n",
      "payload['pipeline_run'] = {'variables': variables}\n",
      "response = requests.post(endpoint, headers=headers, json=payload)\n",
      "return response\n",
      "MageTrigger.trigger_pipeline(\"<pipeline_name>\")\n",
      "Finally, after the mage server is up an running, simply this command:\n",
      "python trigger.py from mage directory in terminal.\n",
      "Can I do data partitioning & clustering run by dbt pipeline, or I would need to do this manually in BigQuery afterwards?\n",
      "You can use this configuration in your DBT model:\n",
      "{\n",
      "\"field\": \"<field name>\",\n",
      "\"data_type\": \"<timestamp | date | datetime | int64>\",\n",
      "\"granularity\": \"<hour | day | month | year>\"\n",
      "# Only required if data_type is \"int64\"\n",
      "\"range\": {\n",
      "\"start\": <int>,\n",
      "\"end\": <int>,\n",
      "\"interval\": <int>\n",
      "}\n",
      "}\n",
      "and for clustering\n",
      "{{\n",
      "config(\n",
      "materialized = \"table\",\n",
      "cluster_by = \"order_id\",\n",
      ")\n",
      "}}\n",
      "more details in: https://docs.getdbt.com/reference/resource-configs/bigquery-configs\n",
      "\n",
      "Section: Workshop 2 - RisingWave\n",
      "Question: Psycopg2 InternalError: Failed to run the query - when running the seed-kafka command after initial setup.\n",
      "Answer: Below I have listed some steps I took to rectify this and potentially other minor errors, in Windows:\n",
      "Use the git bash terminal in windows.\n",
      "Activate python venv from git bash: source .venv/Scripts/activate\n",
      "Modify the seed_kafka.py file: in the first line, replace python3 with python.\n",
      "Now from git bash, run the seed-kafka cmd. It should work now.\n",
      "Additional Notes:\n",
      "You can connect to the RisingWave cluster from Powershell with the command psql -h localhost -p 4566 -d dev -U root , otherwise it asks for a password.\n",
      "The equivalent of source commands.sh  in Powershell is . .\\commands.sh from the workshop directory.\n",
      "Hope this can save you from some trouble in case you're doing this workshop on Windows like I am.\n",
      "—--------------------------------------------------------------------------------------\n",
      "\n",
      "Section: Project\n",
      "Question: How to run python as start up script?\n",
      "Answer: You need to redefine the python environment variable to that of your user account\n",
      "\n",
      "Section: Module 6: streaming with kafka\n",
      "Question: How to fix docker compose error: Error response from daemon: pull access denied for spark-3.3.1, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\n",
      "Answer: If you get this error, know that you have not built your sparks and juypter images. This images aren’t readily available on dockerHub.\n",
      "In the spark folder, run ./build.sh from a bash cli to to build all images before running docker compose\n",
      "\n",
      "Section: Module 6: streaming with kafka\n",
      "Question: Could not start docker image “control-center” from the docker-compose.yaml file.\n",
      "Answer: Check Docker Compose File:\n",
      "Ensure that your docker-compose.yaml file is correctly configured with the necessary details for the \"control-center\" service. Check the service name, image name, ports, volumes, environment variables, and any other configurations required for the container to start.\n",
      "On Mac OSX 12.2.1 (Monterey) I could not start the kafka control center. I opened Docker Desktop and saw docker images still running from week 4, which I did not see when I typed “docker ps.” I deleted them in docker desktop and then had no problem starting up the kafka environment.\n",
      "\n",
      "Section: Workshop 2 - RisingWave\n",
      "Question: Psycopg2 - issues when running stream-kafka script\n",
      "Answer: Replace psycopg2==2.9.9 with psycopg2-binary in the requirements.txt file [source] [another]\n",
      "When you open another terminal to run the psql, remember to do the source command.sh step for each terminal session\n",
      "---------------------------------------------------------------------------------------------\n",
      "\n",
      "Section: Module 5: pyspark\n",
      "Question: How to spark standalone cluster is run on windows OS\n",
      "Answer: Change the working directory to the spark directory:\n",
      "if you have setup up your SPARK_HOME variable, use the following;\n",
      "cd %SPARK_HOME%\n",
      "if not, use the following;\n",
      "cd <path to spark installation>\n",
      "Creating a Local Spark Cluster\n",
      "To start Spark Master:\n",
      "bin\\spark-class org.apache.spark.deploy.master.Master --host localhost\n",
      "Starting up a cluster:\n",
      "bin\\spark-class org.apache.spark.deploy.worker.Worker spark://localhost:7077 --host localhost\n",
      "\n",
      "Section: Module 6: streaming with kafka\n",
      "Question: Error importing cimpl dll when running avro examples\n",
      "Answer: ImportError: DLL load failed while importing cimpl: The specified module could not be found\n",
      "Verify Python Version:\n",
      "Make sure you are using a compatible version of Python with the Avro library. Check the Python version and compatibility requirements specified by the Avro library documentation.\n",
      "... you may have to load librdkafka-5d2e2910.dll in the code. Add this before importing avro:\n",
      "from ctypes import CDLL\n",
      "CDLL(\"C:\\\\Users\\\\YOUR_USER_NAME\\\\anaconda3\\\\envs\\\\dtcde\\\\Lib\\\\site-packages\\\\confluent_kafka.libs\\librdkafka-5d2e2910.dll\")\n",
      "It seems that the error may occur depending on the OS and python version installed.\n",
      "ALTERNATIVE:\n",
      "ImportError: DLL load failed while importing cimpl\n",
      "✅SOLUTION: $env:CONDA_DLL_SEARCH_MODIFICATION_ENABLE=1 in Powershell.\n",
      "You need to set this DLL manually in Conda Env.\n",
      "Source: https://githubhot.com/repo/confluentinc/confluent-kafka-python/issues/1186?page=2\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(create_context(search(q, index)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b81e1190-824c-40ad-bbff-82697dccee42",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_prompt(q, docs_retrieved):\n",
    "\n",
    "    prompt_template=\"\"\"Answer the QUESTION based on the CONTEXT from the FAQ database.\n",
    "    Use only the facts from the CONTEXT when answering the QUESTION.\n",
    "    \n",
    "    QUESTION: \n",
    "    {question}\n",
    "    \n",
    "    CONTEXT: \n",
    "    {context}\n",
    "\n",
    "    \"\"\".strip()\n",
    "\n",
    "    context = create_context(docs_retrieved)\n",
    "\n",
    "    prompt = prompt_template.format(question=q, context=context, date = date.today()).strip()\n",
    "\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9341f3fc-f3c0-44c5-b9dc-230b4e8f8e10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer the QUESTION based on the CONTEXT from the FAQ database.\n",
      "    Use only the facts from the CONTEXT when answering the QUESTION.\n",
      "    \n",
      "    QUESTION: \n",
      "    How do I run Kafka?\n",
      "    \n",
      "    CONTEXT: \n",
      "    Section: Module 6: streaming with kafka\n",
      "Question: Java Kafka: How to run producer/consumer/kstreams/etc in terminal\n",
      "Answer: In the project directory, run:\n",
      "java -cp build/libs/<jar_name>-1.0-SNAPSHOT.jar:out src/main/java/org/example/JsonProducer.java\n",
      "\n",
      "Section: Module 6: streaming with kafka\n",
      "Question: Module “kafka” not found when trying to run producer.py\n",
      "Answer: Solution from Alexey: create a virtual environment and run requirements.txt and the python files in that environment.\n",
      "To create a virtual env and install packages (run only once)\n",
      "python -m venv env\n",
      "source env/bin/activate\n",
      "pip install -r ../requirements.txt\n",
      "To activate it (you'll need to run it every time you need the virtual env):\n",
      "source env/bin/activate\n",
      "To deactivate it:\n",
      "deactivate\n",
      "This works on MacOS, Linux and Windows - but for Windows the path is slightly different (it's env/Scripts/activate)\n",
      "Also the virtual environment should be created only to run the python file. Docker images should first all be up and running.\n",
      "\n",
      "Section: Module 6: streaming with kafka\n",
      "Question: Python Kafka: ./build.sh: Permission denied Error\n",
      "Answer: Run this command in terminal in the same directory (/docker/spark):\n",
      "chmod +x build.sh\n",
      "\n",
      "Section: Module 6: streaming with kafka\n",
      "Question: kafka.errors.NoBrokersAvailable: NoBrokersAvailable\n",
      "Answer: If you have this error, it most likely that your kafka broker docker container is not working.\n",
      "Use docker ps to confirm\n",
      "Then in the docker compose yaml file folder, run docker compose up -d to start all the instances.\n",
      "\n",
      "Section: Module 6: streaming with kafka\n",
      "Question: Python Kafka: ./spark-submit.sh streaming.py - How to check why Spark master connection fails\n",
      "Answer: Start a new terminal\n",
      "Run: docker ps\n",
      "Copy the CONTAINER ID of the spark-master container\n",
      "Run: docker exec -it <spark_master_container_id> bash\n",
      "Run: cat logs/spark-master.out\n",
      "Check for the log when the error happened\n",
      "Google the error message from there\n",
      "\n",
      "Section: Module 6: streaming with kafka\n",
      "Question: How do I check compatibility of local and container Spark versions?\n",
      "Answer: You can check the version of your local spark using spark-submit --version. In the build.sh file of the Python folder, make sure that SPARK_VERSION matches your local version. Similarly, make sure the pyspark you pip installed also matches this version.\n",
      "\n",
      "Section: Module 6: streaming with kafka\n",
      "Question: Python Kafka: Installing dependencies for python3 06-streaming/python/avro_example/producer.py\n",
      "Answer: confluent-kafka: `pip install confluent-kafka` or `conda install conda-forge::python-confluent-kafka`\n",
      "fastavro: pip install fastavro\n",
      "Abhirup Ghosh\n",
      "Can install Faust Library for Module 6 Python Version due to dependency conflicts?\n",
      "The Faust repository and library is no longer maintained - https://github.com/robinhood/faust\n",
      "If you do not know Java, you now have the option to follow the Python Videos 6.13 & 6.14 here https://www.youtube.com/watch?v=BgAlVknDFlQ&list=PL3MmuxUbc_hJed7dXYoJw8DoCuVHhGEQb&index=80  and follow the RedPanda Python version here https://github.com/DataTalksClub/data-engineering-zoomcamp/tree/main/06-streaming/python/redpanda_example - NOTE: I highly recommend watching the Java videos to understand the concept of streaming but you can skip the coding parts - all will become clear when you get to the Python videos and RedPanda files.\n",
      "\n",
      "Section: Workshop 1 - dlthub\n",
      "Question: How do I install the necessary dependencies to run the code?\n",
      "Answer: Answer: To run the provided code, ensure that the 'dlt[duckdb]' package is installed. You can do this by executing the provided installation command: !pip install dlt[duckdb]. If you’re doing it locally, be sure to also have duckdb pip installed (even before the duckdb package is loaded).\n",
      "\n",
      "Section: Module 6: streaming with kafka\n",
      "Question: Java Kafka: Tests are not picked up in VSCode\n",
      "Answer: Situation: in VS Code, usually there will be a triangle icon next to each test. I couldn’t see it at first and had to do some fixes.\n",
      "Solution:\n",
      "(Source)\n",
      "VS Code\n",
      "→ Explorer (first icon on the left navigation bar)\n",
      "→ JAVA PROJECTS (bottom collapsable)\n",
      "→  icon next in the rightmost position to JAVA PROJECTS\n",
      "→  clean Workspace\n",
      "→ Confirm by clicking Reload and Delete\n",
      "Now you will be able to see the triangle icon next to each test like what you normally see in python tests.\n",
      "E.g.:\n",
      "You can also add classes and packages in this window instead of creating files in the project directory\n",
      "\n",
      "Section: Module 6: streaming with kafka\n",
      "Question: Java Kafka: <project_name>-1.0-SNAPSHOT.jar errors: package xxx does not exist even after gradle build\n",
      "Answer: In my set up, all of the dependencies listed in gradle.build were not installed in <project_name>-1.0-SNAPSHOT.jar.\n",
      "Solution:\n",
      "In build.gradle file, I added the following at the end:\n",
      "shadowJar {\n",
      "archiveBaseName = \"java-kafka-rides\"\n",
      "archiveClassifier = ''\n",
      "}\n",
      "And then in the command line ran ‘gradle shadowjar’, and run the script from java-kafka-rides-1.0-SNAPSHOT.jar created by the shadowjar\n",
      "\n",
      "Section: Module 6: streaming with kafka\n",
      "Question: Kafka- python videos have low audio and hard to follow up\n",
      "Answer: tip:As the videos have low audio so I downloaded them and used VLC media player with putting the audio to the max 200% of original audio and the audio became quite good or try to use auto caption generated on Youtube directly.\n",
      "Kafka Python Videos - Rides.csv\n",
      "There is no clear explanation of the rides.csv data that the producer.py python programs use. You can find that here https://raw.githubusercontent.com/DataTalksClub/data-engineering-zoomcamp/2bd33e89906181e424f7b12a299b70b19b7cfcd5/week_6_stream_processing/python/resources/rides.csv.\n",
      "\n",
      "Section: Module 6: streaming with kafka\n",
      "Question: Confluent Kafka: Where can I find schema registry URL?\n",
      "Answer: In Confluent Cloud:\n",
      "Environment → default (or whatever you named your environment as) → The right navigation bar →  “Stream Governance API” →  The URL under “Endpoint”\n",
      "And create credentials from Credentials section below it\n",
      "\n",
      "Section: Module 6: streaming with kafka\n",
      "Question: Java Kafka: When running the producer/consumer/etc java scripts, no results retrieved or no message sent\n",
      "Answer: For example, when running JsonConsumer.java, got:\n",
      "Consuming form kafka started\n",
      "RESULTS:::0\n",
      "RESULTS:::0\n",
      "RESULTS:::0\n",
      "Or when running JsonProducer.java, got:\n",
      "Exception in thread \"main\" java.util.concurrent.ExecutionException: org.apache.kafka.common.errors.SaslAuthenticationException: Authentication failed\n",
      "Solution:\n",
      "Make sure in the scripts in src/main/java/org/example/ that you are running (e.g. JsonConsumer.java, JsonProducer.java), the StreamsConfig.BOOTSTRAP_SERVERS_CONFIG is the correct server url (e.g. europe-west3 from example vs europe-west2)\n",
      "Make sure cluster key and secrets are updated in src/main/java/org/example/Secrets.java (KAFKA_CLUSTER_KEY and KAFKA_CLUSTER_SECRET)\n",
      "\n",
      "Section: Project\n",
      "Question: How to fix the error \"ModuleNotFoundError: No module named 'kafka.vendor.six.moves'\"?\n",
      "Answer: According to https://github.com/dpkp/kafka-python/\n",
      "“DUE TO ISSUES WITH RELEASES, IT IS SUGGESTED TO USE https://github.com/wbarnha/kafka-python-ng FOR THE TIME BEING”\n",
      "Use pip install kafka-python-ng instead\n",
      "\n",
      "Section: Module 6: streaming with kafka\n",
      "Question: Error while running python3 stream.py worker\n",
      "Answer: If you get an error while running the command python3 stream.py worker\n",
      "Run pip uninstall kafka-python\n",
      "Then run pip install kafka-python==1.4.6\n",
      "What is the use of  Redpanda ?\n",
      "Redpanda: Redpanda is built on top of the Raft consensus algorithm and is designed as a high-performance, low-latency alternative to Kafka. It uses a log-centric architecture similar to Kafka but with different underlying principles.\n",
      "Redpanda is a powerful, yet simple, and cost-efficient streaming data platform that is compatible with Kafka® APIs while eliminating Kafka complexity.\n",
      "\n",
      "Section: Module 6: streaming with kafka\n",
      "Question: Kafka homwork Q3, there are options that support scaling concept more than the others:\n",
      "Answer: Ankush said we can focus on horizontal scaling option.\n",
      "“think of scaling in terms of scaling from consumer end. Or consuming message via horizontal scaling”\n",
      "\n",
      "Section: Module 6: streaming with kafka\n",
      "Question: Python Kafka: ‘KafkaTimeoutError: Failed to update metadata after 60.0 secs.’ when running stream-example/producer.py\n",
      "Answer: Restarting all services worked for me:\n",
      "docker-compose down\n",
      "docker-compose up\n",
      "\n",
      "Section: Module 6: streaming with kafka\n",
      "Question: ModuleNotFoundError: No module named 'avro'\n",
      "Answer: ✅SOLUTION: pip install confluent-kafka[avro].\n",
      "For some reason, Conda also doesn't include this when installing confluent-kafka via pip.\n",
      "More sources on Anaconda and confluent-kafka issues:\n",
      "https://github.com/confluentinc/confluent-kafka-python/issues/590\n",
      "https://github.com/confluentinc/confluent-kafka-python/issues/1221\n",
      "https://stackoverflow.com/questions/69085157/cannot-import-producer-from-confluent-kafka\n",
      "\n",
      "Section: Module 6: streaming with kafka\n",
      "Question: Python Kafka: ./spark-submit.sh streaming.py - ERROR StandaloneSchedulerBackend: Application has been killed. Reason: All masters are unresponsive! Giving up.\n",
      "Answer: While following tutorial 13.2 , when running ./spark-submit.sh streaming.py, encountered the following error:\n",
      "…\n",
      "24/03/11 09:48:36 INFO StandaloneAppClient$ClientEndpoint: Connecting to master spark://localhost:7077...\n",
      "24/03/11 09:48:36 INFO TransportClientFactory: Successfully created connection to localhost/127.0.0.1:7077 after 10 ms (0 ms spent in bootstraps)\n",
      "24/03/11 09:48:54 WARN GarbageCollectionMetrics: To enable non-built-in garbage collector(s) List(G1 Concurrent GC), users should configure it(them) to spark.eventLog.gcMetrics.youngGenerationGarbageCollectors or spark.eventLog.gcMetrics.oldGenerationGarbageCollectors\n",
      "24/03/11 09:48:56 INFO StandaloneAppClient$ClientEndpoint: Connecting to master spark://localhost:7077…\n",
      "24/03/11 09:49:16 INFO StandaloneAppClient$ClientEndpoint: Connecting to master spark://localhost:7077...\n",
      "24/03/11 09:49:36 WARN StandaloneSchedulerBackend: Application ID is not initialized yet.\n",
      "24/03/11 09:49:36 ERROR StandaloneSchedulerBackend: Application has been killed. Reason: All masters are unresponsive! Giving up.\n",
      "…\n",
      "py4j.protocol.Py4JJavaError: An error occurred while calling None.org.apache.spark.sql.SparkSession.\n",
      ": java.lang.IllegalStateException: Cannot call methods on a stopped SparkContext.\n",
      "…\n",
      "Solution:\n",
      "Downgrade your local PySpark to 3.3.1 (same as Dockerfile)\n",
      "The reason for the failed connection in my case was the mismatch of PySpark versions. You can see that from the logs of spark-master in the docker container.\n",
      "Solution 2:\n",
      "Check what Spark version your local machine has\n",
      "pyspark –version\n",
      "spark-submit –version\n",
      "Add your version to SPARK_VERSION in build.sh\n",
      "\n",
      "Section: Module 6: streaming with kafka\n",
      "Question: Python Kafka: ./spark-submit.sh streaming.py Error: py4j.protocol.Py4JJavaError: An error occurred while calling None.org.apache.spark.api.java.JavaSparkContext.\n",
      "Answer: Make sure your java version is 11 or 8.\n",
      "Check your version by:\n",
      "java --version\n",
      "Check all your versions by:\n",
      "/usr/libexec/java_home -V\n",
      "If you already have got java 11 but just not selected as default, select the specific version by:\n",
      "export JAVA_HOME=$(/usr/libexec/java_home -v 11.0.22)\n",
      "(or other version of 11)\n",
      "\n",
      "Section: General course-related questions\n",
      "Question: How do I use Git / GitHub for this course?\n",
      "Answer: After you create a GitHub account, you should clone the course repo to your local machine using the process outlined in this video: Git for Everybody: How to Clone a Repository from GitHub\n",
      "Having this local repository on your computer will make it easy for you to access the instructors’ code and make pull requests (if you want to add your own notes or make changes to the course content).\n",
      "You will probably also create your own repositories that host your notes, versions of your file, to do this. Here is a great tutorial that shows you how to do this: https://www.atlassian.com/git/tutorials/setting-up-a-repository\n",
      "Remember to ignore large database, .csv, and .gz files, and other files that should not be saved to a repository. Use .gitignore for this: https://www.atlassian.com/git/tutorials/saving-changes/gitignore NEVER store passwords or keys in a git repo (even if that repo is set to private).\n",
      "This is also a great resource: https://dangitgit.com/\n",
      "\n",
      "Section: Module 4: analytics engineering with dbt\n",
      "Question: BigQuery returns an error when i try to run ‘dbt run’:\n",
      "Answer: My taxi data was loaded into gcs with etl_web_to_gcs.py script that converts csv data into parquet. Then I placed raw data trips into external tables and when I executed dbt run I got an error message: Parquet column 'passenger_count' has type INT64 which does not match the target cpp_type DOUBLE. It is because several columns in files have different formats of data.\n",
      "When I added df[col] = df[col].astype('Int64') transformation to the columns: passenger_count, payment_type, RatecodeID, VendorID, trip_type it went ok. Several people also faced this error and more about it you can read on the slack channel.\n",
      "\n",
      "Section: Triggers in Mage via CLI\n",
      "Question: Encountering the error \"ModuleNotFoundError: No module named 'kafka.vendor.six.moves'\" when running \"from kafka import KafkaProducer\" in Jupyter Notebook for Module 6 Homework?\n",
      "Answer: Python 3.12.1, is not compatible with kafka-python-2.0.2. Therefore, instead of running \"pip install kafka-python\", you can resolve the issue by using \"pip install git+https://github.com/dpkp/kafka-python.git\". If you have already installed kafka-python, you need to run \"pip uninstall kafka-python\" before executing \"pip install git+https://github.com/dpkp/kafka-python.git\" to resolve the compatibility issue.\n",
      "Q:In the Mage pipeline, individual blocks run successfully. However, when executing the pipeline as a whole, some blocks fail.\n",
      "A: I have the following key-value pair in io_config.yaml file configured but still Mage blocks failed to generate OAuth and authenticate with GCP: GOOGLE_SERVICE_ACC_KEY_FILEPATH: \"{{ env_var('GCP_CREDENTIALS') }}\". The GCP_CREDENTIALS variable holds the full path to the service account key's JSON file. Adding the following line within the failed code block resolved the issue: os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = os.environ.get('GCP_CREDENTIALS').\n",
      "This occurs because the path to profiles.yml is not correctly specified. You can rectify this by:\n",
      "“export DBT_PROFILES_DBT=path/to/profiles.yml”\n",
      "Eg., /home/src/magic-zoomcamp/dbt/project_name/\n",
      "Do the similar for DBT_PROJECT_DIR if getting similar issue with dbt_project.yml.\n",
      "Once DIRs are set,:\n",
      "“dbt debug –config-dir”\n",
      "This would update your paths. To maintain same path across sessions, use the path variables in your .env file.\n",
      "To add triggers in mage pipelines via CLI, you can create a trigger of type API, and copy the API links.\n",
      "Eg. link: http://localhost:6789/api/pipeline_schedules/10/pipeline_runs/f3a1a4228fc64cfd85295b668c93f3b2\n",
      "Then create a trigger.py as such:\n",
      "import os\n",
      "import requests\n",
      "class MageTrigger:\n",
      "OPTIONS = {\n",
      "\"<pipeline_name>\": {\n",
      "\"trigger_id\": 10,\n",
      "\"key\": \"f3a1a4228fc64cfd85295b668c93f3b2\"\n",
      "}\n",
      "}\n",
      "@staticmethod\n",
      "def trigger_pipeline(pipeline_name, variables=None):\n",
      "trigger_id = MageTrigger.OPTIONS[pipeline_name][\"trigger_id\"]\n",
      "key = MageTrigger.OPTIONS[pipeline_name][\"key\"]\n",
      "endpoint = f\"http://localhost:6789/api/pipeline_schedules/{trigger_id}/pipeline_runs/{key}\"\n",
      "headers = {'Content-Type': 'application/json'}\n",
      "payload = {}\n",
      "if variables is not None:\n",
      "payload['pipeline_run'] = {'variables': variables}\n",
      "response = requests.post(endpoint, headers=headers, json=payload)\n",
      "return response\n",
      "MageTrigger.trigger_pipeline(\"<pipeline_name>\")\n",
      "Finally, after the mage server is up an running, simply this command:\n",
      "python trigger.py from mage directory in terminal.\n",
      "Can I do data partitioning & clustering run by dbt pipeline, or I would need to do this manually in BigQuery afterwards?\n",
      "You can use this configuration in your DBT model:\n",
      "{\n",
      "\"field\": \"<field name>\",\n",
      "\"data_type\": \"<timestamp | date | datetime | int64>\",\n",
      "\"granularity\": \"<hour | day | month | year>\"\n",
      "# Only required if data_type is \"int64\"\n",
      "\"range\": {\n",
      "\"start\": <int>,\n",
      "\"end\": <int>,\n",
      "\"interval\": <int>\n",
      "}\n",
      "}\n",
      "and for clustering\n",
      "{{\n",
      "config(\n",
      "materialized = \"table\",\n",
      "cluster_by = \"order_id\",\n",
      ")\n",
      "}}\n",
      "more details in: https://docs.getdbt.com/reference/resource-configs/bigquery-configs\n",
      "\n",
      "Section: Workshop 2 - RisingWave\n",
      "Question: Psycopg2 InternalError: Failed to run the query - when running the seed-kafka command after initial setup.\n",
      "Answer: Below I have listed some steps I took to rectify this and potentially other minor errors, in Windows:\n",
      "Use the git bash terminal in windows.\n",
      "Activate python venv from git bash: source .venv/Scripts/activate\n",
      "Modify the seed_kafka.py file: in the first line, replace python3 with python.\n",
      "Now from git bash, run the seed-kafka cmd. It should work now.\n",
      "Additional Notes:\n",
      "You can connect to the RisingWave cluster from Powershell with the command psql -h localhost -p 4566 -d dev -U root , otherwise it asks for a password.\n",
      "The equivalent of source commands.sh  in Powershell is . .\\commands.sh from the workshop directory.\n",
      "Hope this can save you from some trouble in case you're doing this workshop on Windows like I am.\n",
      "—--------------------------------------------------------------------------------------\n",
      "\n",
      "Section: Project\n",
      "Question: How to run python as start up script?\n",
      "Answer: You need to redefine the python environment variable to that of your user account\n",
      "\n",
      "Section: Module 6: streaming with kafka\n",
      "Question: How to fix docker compose error: Error response from daemon: pull access denied for spark-3.3.1, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\n",
      "Answer: If you get this error, know that you have not built your sparks and juypter images. This images aren’t readily available on dockerHub.\n",
      "In the spark folder, run ./build.sh from a bash cli to to build all images before running docker compose\n",
      "\n",
      "Section: Module 6: streaming with kafka\n",
      "Question: Could not start docker image “control-center” from the docker-compose.yaml file.\n",
      "Answer: Check Docker Compose File:\n",
      "Ensure that your docker-compose.yaml file is correctly configured with the necessary details for the \"control-center\" service. Check the service name, image name, ports, volumes, environment variables, and any other configurations required for the container to start.\n",
      "On Mac OSX 12.2.1 (Monterey) I could not start the kafka control center. I opened Docker Desktop and saw docker images still running from week 4, which I did not see when I typed “docker ps.” I deleted them in docker desktop and then had no problem starting up the kafka environment.\n",
      "\n",
      "Section: Workshop 2 - RisingWave\n",
      "Question: Psycopg2 - issues when running stream-kafka script\n",
      "Answer: Replace psycopg2==2.9.9 with psycopg2-binary in the requirements.txt file [source] [another]\n",
      "When you open another terminal to run the psql, remember to do the source command.sh step for each terminal session\n",
      "---------------------------------------------------------------------------------------------\n",
      "\n",
      "Section: Module 5: pyspark\n",
      "Question: How to spark standalone cluster is run on windows OS\n",
      "Answer: Change the working directory to the spark directory:\n",
      "if you have setup up your SPARK_HOME variable, use the following;\n",
      "cd %SPARK_HOME%\n",
      "if not, use the following;\n",
      "cd <path to spark installation>\n",
      "Creating a Local Spark Cluster\n",
      "To start Spark Master:\n",
      "bin\\spark-class org.apache.spark.deploy.master.Master --host localhost\n",
      "Starting up a cluster:\n",
      "bin\\spark-class org.apache.spark.deploy.worker.Worker spark://localhost:7077 --host localhost\n",
      "\n",
      "Section: Module 6: streaming with kafka\n",
      "Question: Error importing cimpl dll when running avro examples\n",
      "Answer: ImportError: DLL load failed while importing cimpl: The specified module could not be found\n",
      "Verify Python Version:\n",
      "Make sure you are using a compatible version of Python with the Avro library. Check the Python version and compatibility requirements specified by the Avro library documentation.\n",
      "... you may have to load librdkafka-5d2e2910.dll in the code. Add this before importing avro:\n",
      "from ctypes import CDLL\n",
      "CDLL(\"C:\\\\Users\\\\YOUR_USER_NAME\\\\anaconda3\\\\envs\\\\dtcde\\\\Lib\\\\site-packages\\\\confluent_kafka.libs\\librdkafka-5d2e2910.dll\")\n",
      "It seems that the error may occur depending on the OS and python version installed.\n",
      "ALTERNATIVE:\n",
      "ImportError: DLL load failed while importing cimpl\n",
      "✅SOLUTION: $env:CONDA_DLL_SEARCH_MODIFICATION_ENABLE=1 in Powershell.\n",
      "You need to set this DLL manually in Conda Env.\n",
      "Source: https://githubhot.com/repo/confluentinc/confluent-kafka-python/issues/1186?page=2\n"
     ]
    }
   ],
   "source": [
    "print(create_prompt(q, search(q, index)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b7ffe04a-365b-46b5-9914-9b26315aa728",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_response_llm_context(client, index, q, model):\n",
    "\n",
    "    docs_retrieved = search(q, index)\n",
    "    prompt = create_prompt(q, docs_retrieved)\n",
    "\n",
    "    response = get_response(client = client, model = model, system_context=\"\", assistant_context=\"\", user_context=prompt)\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d328a0b0-9830-4d1f-abe3-f9d93cbf4095",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To run Kafka, for a Java application, in the project directory, the command to be used is:\n",
      "java -cp build/libs/<jar_name>-1.0-SNAPSHOT.jar:out src/main/java/org/example/JsonProducer.java \n",
      "\n",
      "For a Python Kafka application, you will need to create a virtual environment and run the Python files inside that environment. The steps involved are:\n",
      "1. To create a virtual env and install packages, run these commands just once:\n",
      "```\n",
      "python -m venv env\n",
      "source env/bin/activate\n",
      "pip install -r ../requirements.txt\n",
      "```\n",
      "2. For activating the environment, you need to run this every time:\n",
      "```\n",
      "source env/bin/activate\n",
      "```\n",
      "3. To deactivate the environment, run:\n",
      "```\n",
      "deactivate\n",
      "```\n",
      "\n",
      "For a Python Kafka application, before running the streaming.py script, you may need to ensure you have the necessary dependencies installed. To install the dependencies needed for Python3 script 06-streaming/python/avro_example/producer.py, run the following commands:\n",
      "```\n",
      "confluent-kafka: pip install confluent-kafka or conda install conda-forge::python-confluent-kafka\n",
      "fastavro: pip install fastavro\n",
      "```\n",
      "\n",
      "Redpanda is an alternative to Kafka that is built on top of the Raft consensus algorithm. It provides a high-performance, low-latency solution for streaming data similar to Kafka but with different underlying principles. Redpanda also eliminates the complexity associated with Kafka.\n",
      "\n",
      "To fix compatibility issues with Kafka and Python, replace the kafka-python package with kafka-python-ng by executing:\n",
      "```\n",
      "pip install kafka-python-ng\n",
      "```\n",
      "\n",
      "Additionally, to fix an error related to Java version when running a Python script in Spark, ensure that your Java version is either 11 or 8. You can set the specific version using the JAVA_HOME environment variable.\n",
      "\n",
      "Remember to troubleshoot system errors based on the best practices provided within the explanations and instructions to ensure a smooth running of your Kafka applications.\n"
     ]
    }
   ],
   "source": [
    "print(get_response_llm_context(client, index, q, 'gpt-3.5-turbo'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "200d09ac-8538-4dfd-b64c-dbcce11fffff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from elasticsearch import Elasticsearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "34993d2d-62ac-43be-841b-7a24ef6834a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "es_client = Elasticsearch(\"http://localhost:9200\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "788e51a2-7a9d-4e5d-a440-2fdb1fe5f915",
   "metadata": {},
   "outputs": [],
   "source": [
    "index_anatomy = {\n",
    "    \"settings\": {\n",
    "        \"number_of_shards\": 1,\n",
    "        \"number_of_replicas\": 0\n",
    "    },\n",
    "    \"mappings\": {\n",
    "        \"properties\": {\n",
    "            \"text\": {\"type\": \"text\"},\n",
    "            \"section\": {\"type\": \"text\"},\n",
    "            \"question\": {\"type\": \"text\"},\n",
    "            \"course\": {\"type\": \"keyword\"} \n",
    "        }\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a5db49b0-b600-4c2a-9a4e-af05bc86e919",
   "metadata": {},
   "outputs": [],
   "source": [
    "index_name = \"course-questions\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3c76c9c2-c817-4630-af04-b51e9840ac7e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ObjectApiResponse({'acknowledged': True, 'shards_acknowledged': True, 'index': 'course-questions'})"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "es_client.indices.create(index=index_name,\n",
    "                         body=index_anatomy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fa43bb9b-d716-4cf4-8612-d17a9f524a96",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/python/3.10.13/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "100%|████████████████████████████████████████████████████████████| 948/948 [00:29<00:00, 31.62it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "for doc in tqdm(docs):\n",
    "    es_client.index(index=index_name, document=doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "65972ec7-39c4-4645-89f7-b555236cf2f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Can I still enroll it?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "81d070ce-1f80-4fcb-9a55-8e997e348f5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search(q, es_client):\n",
    "\n",
    "    search_query = {\n",
    "    \"size\": 5,\n",
    "    \"query\": {\n",
    "        \"bool\": {\n",
    "            \"must\": {\n",
    "                \"multi_match\": {\n",
    "                    \"query\": q,\n",
    "                    \"fields\": [\"question^3\", \"text\", \"section\"],\n",
    "                    \"type\": \"best_fields\"\n",
    "                }\n",
    "            },\n",
    "            \"filter\": {\n",
    "                \"term\": {\n",
    "                    \"course\": \"data-engineering-zoomcamp\"\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "\n",
    "    docs_retrieved = []\n",
    "    for hit in es_client.search(index='course-questions', body=search_query)['hits']['hits']:\n",
    "        docs_retrieved.append(hit['_source'])\n",
    "\n",
    "    return docs_retrieved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "3ee4e52b-0b44-466e-9038-152e9ba6418c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'text': \"Yes, even if you don't register, you're still eligible to submit the homeworks.\\nBe aware, however, that there will be deadlines for turning in the final projects. So don't leave everything for the last minute.\",\n",
       "  'section': 'General course-related questions',\n",
       "  'question': 'Course - Can I still join the course after the start date?',\n",
       "  'course': 'data-engineering-zoomcamp'},\n",
       " {'text': 'Yes, we will keep all the materials after the course finishes, so you can follow the course at your own pace after it finishes.\\nYou can also continue looking at the homeworks and continue preparing for the next cohort. I guess you can also start working on your final capstone project.',\n",
       "  'section': 'General course-related questions',\n",
       "  'question': 'Course - Can I follow the course after it finishes?',\n",
       "  'course': 'data-engineering-zoomcamp'},\n",
       " {'text': 'Create a new branch for development, then you can merge it to the main branch\\nCreate a new branch and switch to this branch. It allows you to make changes. Then you can commit and push the changes to the “main” branch.',\n",
       "  'section': 'Module 4: analytics engineering with dbt',\n",
       "  'question': \"Dbt+git - It appears that I can't edit the files because I'm in read-only mode. Does anyone know how I can change that?\",\n",
       "  'course': 'data-engineering-zoomcamp'},\n",
       " {'text': 'Remove the dataset from BigQuery which was created by dbt and run dbt run again so that it will recreate the dataset in BigQuery with the correct location',\n",
       "  'section': 'Module 4: analytics engineering with dbt',\n",
       "  'question': 'I changed location in dbt, but dbt run still gives me an error',\n",
       "  'course': 'data-engineering-zoomcamp'},\n",
       " {'text': 'Yes, you can use any tool you want for your project.',\n",
       "  'section': 'General course-related questions',\n",
       "  'question': 'Can I use Airflow instead for my final project?',\n",
       "  'course': 'data-engineering-zoomcamp'}]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search(q=query, es_client=es_client)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "c34e826e-d2a4-48f3-bc95-06b76d859b53",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_response_llm_context(client, es_client, q, model):\n",
    "\n",
    "    docs_retrieved = search(q, es_client)\n",
    "    prompt = create_prompt(q, docs_retrieved)\n",
    "\n",
    "    response = get_response(client = client, model = model, system_context=\"\", assistant_context=\"\", user_context=prompt)\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "66e2a839-3bf3-4887-8bd9-456ed7d21f35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No, you cannot enroll after the start date. However, you can still submit homeworks and access course materials at your own pace after the course finishes.\n"
     ]
    }
   ],
   "source": [
    "print(get_response_llm_context(client, es_client, q=query, model=\"gpt-3.5-turbo\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
